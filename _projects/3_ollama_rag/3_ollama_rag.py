from langchain_ollama.llms import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate
from create_vectors import retriever


model = OllamaLLM(model="phi4:latest") 

template = """
Eres un investigador experto en filosof√≠a, sociolog√≠a digital y an√°lisis de datos. Est√°s trabajando en un proyecto titulado "La Generaci√≥n Z y la Crisis de Sentido en la Era Digital".

Tu objetivo es responder preguntas sintetizando dos tipos de informaci√≥n que se te proporcionar√°n:
1. MARCO TE√ìRICO: Conceptos filos√≥ficos (Existencialismo, Posmodernidad, Foucault, Byung-Chul Han, etc.).
2. EVIDENCIA EMP√çRICA: Datos de redes sociales, comentarios de YouTube y encuestas sint√©ticas.

Instrucciones:
- Utiliza la informaci√≥n proporcionada en el CONTEXTO para responder.
- Siempre intenta conectar la teor√≠a (ej. "Vac√≠o existencial") con la evidencia real (ej. "Comentarios de usuarios").
- Si la informaci√≥n no est√° en el contexto, di que no tienes datos suficientes, no inventes.
- Responde siempre en Espa√±ol formal y acad√©mico pero accesible.

CONTEXTO RECUPERADO:
{context}

PREGUNTA DEL USUARIO: 
{question}

RESPUESTA (An√°lisis estructurado):
"""

prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model


print("\n~~~~~~~~~~~ Proyecto 3 (RAG): GEN Z & IA ~~~~~~~~~~")
print("\n=== La Generaci√≥n Z y la Crisis de Sentido en la Era Digital ===")
print("\n=== Tecnolog√≠a, Inteligencia Artificial y la Desaparici√≥n de la Autonom√≠a Humana ===")
def chat_loop():
    
    while True:
        print("\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~")
        question = input("Pregunta/Consulta: ")
        
        if question.lower() in ["salir", "exit", "q"]:
            break
        
        print("Buscando evidencia y teor√≠a...")
        
        # Recuperar informacion importante
        docs = retriever.invoke(question)
        
        # Formatear el contexto para que el LLM sepa qu√© es qu√©
        context_text = ""
        for doc in docs:
            source = doc.metadata.get("source", "desconocido")
            content = doc.page_content
            # Etiquetamos el origen para que el LLM sepa distinguir teor√≠a de opini√≥n
            context_text += f"[{source.upper()}]: {content}\n\n"
        
        
        print("Pensando...\n")
        result = chain.invoke({"context": context_text, "question": question})
        print(result)

        print("\n--- FUENTES UTILIZADAS (EVIDENCIA REAL) ---")
        seen_sources = set()
        for doc in docs:
            meta = doc.metadata
            source_type = meta.get("source", "Desconocido")

            source_label = f"[{source_type.upper()}]"

            details = ""
            if source_type == "youtube_comentarios":
                details = f"Autor: {meta.get('author', 'Anon')} | Likes: {meta.get('like_count', 0)}"
            elif source_type == "teoria_filosofica":
                details = f"Autor: {meta.get('autor_concepto', 'N/A')} | Eje: {meta.get('eje_analisis', 'N/A')}"
            elif source_type == "articulos_externos":
                details = f"T√≠tulo: {meta.get('title', 'Sin t√≠tulo')}"

            identifier = f"{source_label} {details}"
            if identifier not in seen_sources:
                print(f"üìÑ {source_label} {details}")
                # Opcional: Si quieres ver el fragmento de texto exacto, descomenta la linea de abajo
                # print(f"   Fragmento: {doc.page_content[:100]}...") 
                seen_sources.add(identifier)

if __name__ == "__main__":
    chat_loop()